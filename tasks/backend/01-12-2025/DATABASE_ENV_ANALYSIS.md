# Analisis Konfigurasi Database di `.env` - REDUNDANCY DETECTED âš ï¸

**Date**: 2025-12-01  
**File**: `/app/backend/.env`  
**Scope**: Database connection configuration  
**Status**: âš ï¸ ADA REDUNDANSI - PERLU CLEANUP

---

## ðŸ“‹ CURRENT CONFIGURATION

### 3 Database URLs Defined:

```env
# 1. DATABASE_URL (PRIMARY - USED BY APP)
DATABASE_URL="postgresql://postgres.ascucdkibziqamaaovqw:123Empatlima@aws-1-ap-southeast-2.pooler.supabase.com:6543/postgres?pgbouncer=true&connection_limit=1"

# 2. DIRECT_URL (UNUSED - REDUNDANT âŒ)
DIRECT_URL="postgresql://postgres.ascucdkibziqamaaovqw:123Empatlima@aws-1-ap-southeast-2.pooler.supabase.com:5432/postgres"

# 3. SUPABASE_DATABASE_URL (BACKUP - MINIMAL USAGE)
SUPABASE_DATABASE_URL="postgresql://postgres.ascucdkibziqamaaovqw:123Empatlima@aws-1-ap-southeast-2.pooler.supabase.com:5432/postgres"
```

---

## ðŸ” ANALISIS MENDALAM

### 1. `DATABASE_URL` - PRIMARY CONNECTION âœ…

**Port**: `6543` (PgBouncer Connection Pooler)  
**Parameters**: `pgbouncer=true&connection_limit=1`  
**Usage**: **PRIMARY** - Digunakan oleh seluruh aplikasi

**Digunakan Oleh**:
- âœ… Prisma Client (`schema.prisma` â†’ `url = env("DATABASE_URL")`)
- âœ… Backend API (`src/index.ts` â†’ Prisma menggunakan DATABASE_URL)
- âœ… Worker (`src/worker.ts` â†’ Validasi `process.env.DATABASE_URL`)
- âœ… Migration script (`src/scripts/migrate-data.ts` â†’ Fallback)

**Karakteristik**:
- **Connection Pooling**: PgBouncer mengelola koneksi database
- **Scalability**: Cocok untuk high concurrency (serverless, API dengan banyak request)
- **Limitation**: `connection_limit=1` â†’ Setiap Prisma Client instance max 1 koneksi
- **Transaction Mode**: PgBouncer dalam mode "transaction pooling"

**Kapan Gunakan**:
- âœ… Production API server
- âœ… Serverless functions (Vercel, AWS Lambda)
- âœ… High traffic scenarios
- âŒ Long-running transactions (gunakan DIRECT_URL)
- âŒ Prisma migrations (gunakan DIRECT_URL)

---

### 2. `DIRECT_URL` - COMPLETELY UNUSED âŒ

**Port**: `5432` (Direct PostgreSQL Connection)  
**Parameters**: None  
**Usage**: **TIDAK DIGUNAKAN SAMA SEKALI**

**Not Referenced Anywhere**:
```bash
# Search result: 0 matches
grep -r "DIRECT_URL" /app/backend/src/
# No results
```

**Status**: âŒ **REDUNDANT - DAPAT DIHAPUS**

**Intended Use** (Prisma Best Practice):
Prisma merekomendasikan 2 URLs untuk production:
- `DATABASE_URL` â†’ Pooled connection (untuk queries)
- `DIRECT_URL` â†’ Direct connection (untuk migrations)

**Problem**: Di codebase ini, `DIRECT_URL` didefinisikan tapi **tidak pernah digunakan**.

**Schema Prisma Seharusnya**:
```prisma
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")       // Pooled
  directUrl = env("DIRECT_URL")         // Direct (untuk migrations)
}
```

**Schema Prisma SAAT INI**:
```prisma
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")       // Hanya ini
  // directUrl TIDAK DIDEFINISIKAN âŒ
}
```

**Consequence**: Migrations berjalan melalui PgBouncer (port 6543), yang bisa bermasalah jika migration memiliki:
- Long-running transactions
- Schema changes yang kompleks
- DDL statements yang membutuhkan session pool

---

### 3. `SUPABASE_DATABASE_URL` - MINIMAL USAGE âš ï¸

**Port**: `5432` (Direct PostgreSQL Connection)  
**Parameters**: None  
**Usage**: **HANYA 1 FILE** - `/app/backend/src/scripts/migrate-data.ts`

**Digunakan Oleh**:
```typescript
// src/scripts/migrate-data.ts:19
const SUPABASE_DB_URL: string = 
  process.env.SUPABASE_DATABASE_URL || 
  process.env.DATABASE_URL || 
  ''
```

**Purpose**: 
- Script manual untuk migrasi data dari local â†’ Supabase
- Bukan part of normal application flow
- One-time migration script

**Karakteristik**:
- **Direct Connection**: Tidak melalui pooler
- **Full Features**: Support semua PostgreSQL features
- **No Limitations**: Bisa jalankan complex queries, long transactions

**Status**: âš ï¸ **SEMI-REDUNDANT**  
- Technically redundant dengan `DIRECT_URL` (nilai sama)
- Tapi digunakan di migration script
- Bisa diganti dengan `DIRECT_URL` jika `DIRECT_URL` diaktifkan

---

## ðŸŽ¯ ANALISIS REDUNDANSI

### Redundancy Matrix

| Variable | Port | Type | Used By | Status |
|----------|------|------|---------|--------|
| `DATABASE_URL` | 6543 | Pooled | Prisma, API, Worker | âœ… **ACTIVE** |
| `DIRECT_URL` | 5432 | Direct | **NOTHING** | âŒ **UNUSED** |
| `SUPABASE_DATABASE_URL` | 5432 | Direct | migrate-data.ts | âš ï¸ **MINIMAL** |

### Problem Summary

1. **`DIRECT_URL` dan `SUPABASE_DATABASE_URL` IDENTIK**
   - Sama persis (host, port, credentials)
   - Nilai duplikat â†’ waste memory, confusing

2. **`DIRECT_URL` tidak digunakan**
   - Didefinisikan tapi tidak direferensikan
   - Dead configuration

3. **`SUPABASE_DATABASE_URL` hanya 1 script**
   - Digunakan di migration script one-time
   - Bisa diganti dengan `DIRECT_URL`

4. **Prisma schema tidak optimal**
   - Tidak menggunakan `directUrl` untuk migrations
   - Migrations run through PgBouncer (risky)

---

## âš ï¸ RISIKO CURRENT CONFIGURATION

### 1. Migration Issues (MEDIUM RISK)

**Problem**: Migrations berjalan melalui PgBouncer (port 6543)

**Potential Issues**:
```sql
-- DDL statements yang kompleks bisa timeout
ALTER TABLE "User" ADD COLUMN new_field VARCHAR(255);
CREATE INDEX CONCURRENTLY idx_email ON "User" (email);

-- PgBouncer bisa drop connection di tengah-tengah
-- Karena transaction pooling mode
```

**Impact**:
- Migration bisa gagal di tengah jalan
- Partial schema changes (dangerous!)
- Rollback sulit

**Mitigation**: 
- Gunakan `DIRECT_URL` untuk migrations
- Update `schema.prisma` dengan `directUrl`

---

### 2. Long Transactions (LOW RISK)

**Problem**: API queries melalui PgBouncer dengan `connection_limit=1`

**Potential Issues**:
```typescript
// Long transaction bisa di-kill PgBouncer
await prisma.$transaction(async (tx) => {
  // Complex multi-table operations
  await tx.user.update(...)
  await tx.order.create(...)
  await tx.notification.create(...)
  // Jika > 30 detik, PgBouncer bisa timeout
})
```

**Impact**:
- Transactions di-interrupt
- Data inconsistency risk
- User errors (500)

**Mitigation**:
- Keep transactions short
- Use queue for long operations
- Consider direct connection for batch jobs

---

### 3. Connection Pool Exhaustion (LOW RISK)

**Problem**: `connection_limit=1` per Prisma instance

**Scenario**:
```
Backend API running:
- Request 1: Query 1 (using connection)
- Request 2: Query 2 (waiting for connection)
- Request 3: Query 3 (waiting...)
```

**Impact**:
- Slow response times
- Queue buildup
- User timeouts

**Current Status**: âœ… OK for current traffic (early stage)  
**Future Risk**: High traffic akan hit limit

**Mitigation**:
- Increase `connection_limit=10` (Supabase pooler default)
- Monitor connection usage
- Scale horizontally (multiple API instances)

---

## âœ… REKOMENDASI - CLEANUP & OPTIMIZATION

### Option 1: CLEANUP MINIMAL (QUICK FIX)

**Tujuan**: Hapus redundansi, keep functionality

**Action**:
```env
# KEEP - Primary connection untuk app
DATABASE_URL="postgresql://postgres.ascucdkibziqamaaovqw:123Empatlima@aws-1-ap-southeast-2.pooler.supabase.com:6543/postgres?pgbouncer=true&connection_limit=1"

# REMOVE - Tidak digunakan sama sekali
# DIRECT_URL="..." âŒ DELETE

# KEEP - Digunakan di migration script (atau rename jadi DIRECT_URL)
DIRECT_URL="postgresql://postgres.ascucdkibziqamaaovqw:123Empatlima@aws-1-ap-southeast-2.pooler.supabase.com:5432/postgres"
```

**Changes**:
1. Hapus `SUPABASE_DATABASE_URL`
2. Ganti `DIRECT_URL` dengan nilai yang sama
3. Update migration script:
   ```typescript
   // src/scripts/migrate-data.ts
   const SUPABASE_DB_URL: string = 
     process.env.DIRECT_URL ||  // âœ… Changed
     process.env.DATABASE_URL || 
     ''
   ```

**Benefit**:
- âœ… No redundancy
- âœ… Cleaner .env
- âœ… Minimal code changes

---

### Option 2: OPTIMAL PRISMA SETUP (RECOMMENDED)

**Tujuan**: Follow Prisma best practices untuk production

**Action**:

**1. Update `.env`:**
```env
# Pooled connection (untuk API queries)
DATABASE_URL="postgresql://postgres.ascucdkibziqamaaovqw:123Empatlima@aws-1-ap-southeast-2.pooler.supabase.com:6543/postgres?pgbouncer=true&connection_limit=10"

# Direct connection (untuk migrations & batch jobs)
DIRECT_URL="postgresql://postgres.ascucdkibziqamaaovqw:123Empatlima@aws-1-ap-southeast-2.pooler.supabase.com:5432/postgres"
```

**2. Update `prisma/schema.prisma`:**
```prisma
datasource db {
  provider  = "postgresql"
  url       = env("DATABASE_URL")   // Pooled untuk queries
  directUrl = env("DIRECT_URL")     // Direct untuk migrations
}
```

**3. Update migration script:**
```typescript
// src/scripts/migrate-data.ts
const SUPABASE_DB_URL: string = 
  process.env.DIRECT_URL ||        // âœ… Use DIRECT_URL
  process.env.DATABASE_URL || 
  ''
```

**4. Increase connection limit:**
```env
# Change from connection_limit=1 â†’ 10
DATABASE_URL="...?pgbouncer=true&connection_limit=10"
```

**Benefit**:
- âœ… Migrations run safely (direct connection)
- âœ… API queries fast (pooled connection)
- âœ… Better scalability (higher connection limit)
- âœ… Follows Prisma official docs
- âœ… Production-ready setup

---

### Option 3: KEEP AS-IS (NOT RECOMMENDED)

**If you choose to keep current setup**:

**Risks to Monitor**:
1. âš ï¸ Watch for migration failures
2. âš ï¸ Monitor PgBouncer timeouts
3. âš ï¸ Check connection pool exhaustion

**When to Change**:
- When traffic increases (> 100 concurrent users)
- When migrations fail
- When seeing connection timeout errors

---

## ðŸ“Š COMPARISON TABLE

| Aspect | Current Setup | Option 1 (Cleanup) | Option 2 (Optimal) |
|--------|---------------|-------------------|-------------------|
| **Redundancy** | âŒ 3 URLs, 2 duplikat | âœ… 2 URLs, no duplikat | âœ… 2 URLs, no duplikat |
| **Migration Safety** | âš ï¸ Via PgBouncer | âš ï¸ Via PgBouncer | âœ… Direct connection |
| **API Performance** | âœ… Pooled | âœ… Pooled | âœ… Pooled (higher limit) |
| **Scalability** | âš ï¸ Limited (1 conn) | âš ï¸ Limited (1 conn) | âœ… Better (10 conns) |
| **Code Changes** | None | Minimal (1 file) | Moderate (3 files) |
| **Production Ready** | âš ï¸ Medium | âš ï¸ Medium | âœ… High |

---

## ðŸŽ¯ FINAL RECOMMENDATION

### âœ… **PILIH OPTION 2: OPTIMAL PRISMA SETUP**

**Alasan**:
1. âœ… Eliminates redundancy
2. âœ… Follows Prisma best practices
3. âœ… Safer migrations (direct connection)
4. âœ… Better scalability (10 connections)
5. âœ… Production-ready architecture
6. âœ… Minimal effort (3 file changes)

**Implementation Steps**:
1. Backup `.env` file
2. Update `DATABASE_URL` â†’ increase `connection_limit=10`
3. Keep `DIRECT_URL` dengan port 5432
4. Remove `SUPABASE_DATABASE_URL`
5. Update `schema.prisma` â†’ add `directUrl`
6. Update `migrate-data.ts` â†’ use `DIRECT_URL`
7. Test migrations: `npm run db:migrate`
8. Test API: Check all endpoints work
9. Monitor logs for 24 hours

**Risk Level**: LOW  
**Effort**: 30 minutes  
**Impact**: HIGH (better performance + reliability)

---

## ðŸ“ ACTION ITEMS

- [ ] Decision: Choose Option 1, 2, or 3
- [ ] If Option 2: Update `.env` file
- [ ] If Option 2: Update `schema.prisma`
- [ ] If Option 2: Update `migrate-data.ts`
- [ ] Test migrations: `npx prisma migrate deploy`
- [ ] Test API endpoints
- [ ] Monitor logs for errors
- [ ] Update documentation

---

---

## ðŸŽ¨ FRONTEND ENV ANALYSIS

**Files Checked**:
- `/app/frontend/.env.local` (35 bytes - only MAINTENANCE_MODE)
- `/app/frontend/.env.production` (898 bytes - production config)

### Supabase Configuration - CLEAN âœ…

```env
NEXT_PUBLIC_SUPABASE_URL=https://ascucdkibziqamaaovqw.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
NEXT_PUBLIC_SUPABASE_REDIRECT_URL=https://jastipin.me/auth/callback
```

### Usage Analysis

**Files Using Supabase** (6 files):
1. âœ… `lib/supabase-client.ts` - Browser client creation
2. âœ… `lib/supabase-auth-context.tsx` - Auth state management
3. âœ… `app/auth/callback/route.ts` - OAuth callback handler
4. âœ… `components/auth/google-login-button.tsx` - Google OAuth trigger
5. âœ… `components/auth/oauth-session-handler.tsx` - Session handler
6. âœ… `app/layout.tsx` - Auth provider wrapper

**Environment Variables Usage**:
```typescript
// lib/supabase-client.ts
createBrowserClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
)

// app/auth/callback/route.ts
createServerClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
  { cookies: { ... } }
)
```

### Redundancy Check: NONE âœ…

- âœ… **No duplicate variables**
- âœ… **All 3 variables used actively**
- âœ… **Clean architecture**
- âœ… **Follows Next.js conventions (NEXT_PUBLIC_ prefix)**

**Verdict**: Frontend env configuration is **OPTIMAL** - no cleanup needed.

---

## ðŸ”„ MIGRATION FLOW: SWITCH TO NEW SUPABASE INSTANCE

### Prerequisites Checklist

- [ ] New Supabase project created
- [ ] Database schema deployed to new instance
- [ ] Data migrated (if applicable)
- [ ] OAuth credentials configured in new Supabase
- [ ] DNS/domain verified for redirect URLs

---

### STEP-BY-STEP MIGRATION FLOW

#### ðŸ”´ PHASE 0: PREPARATION (30 minutes)

**0.1. Create New Supabase Project**
```bash
# 1. Go to https://app.supabase.com
# 2. Click "New Project"
# 3. Fill in:
#    - Name: Jastipin_02
#    - Region: ap-southeast-2 (Sydney) - SAME as current
#    - Database Password: [Generate strong password]
# 4. Wait 2-3 minutes for provisioning
```

**0.2. Get New Credentials**
```bash
# In new Supabase dashboard:
# Settings â†’ API â†’ Copy these:

NEW_PROJECT_URL=https://NEWPROJECTID.supabase.co
NEW_ANON_KEY=eyJhbGci...  (JWT token)
NEW_SERVICE_ROLE_KEY=eyJhbGci...  (Keep secret!)
NEW_DATABASE_PASSWORD=[from setup]
```

**0.3. Backup Current Database**
```bash
# Option 1: Supabase Dashboard
# Dashboard â†’ Database â†’ Backups â†’ "Create backup now"

# Option 2: Manual pg_dump
cd /app/backend
export $(cat .env | grep "^DATABASE_URL=" | xargs)
pg_dump "$DATABASE_URL" > backup_$(date +%Y%m%d_%H%M%S).sql
```

---

#### ðŸŸ¡ PHASE 1: DEPLOY SCHEMA TO NEW INSTANCE (15 minutes)

**1.1. Update Backend .env (Temporary Dual Config)**
```bash
cd /app/backend
cp .env .env.backup

# Edit .env - ADD new instance as secondary
cat >> .env << 'EOF'

# NEW SUPABASE INSTANCE (MIGRATION TARGET)
NEW_DATABASE_URL="postgresql://postgres.NEWPROJECTID:NEWPASSWORD@aws-1-ap-southeast-2.pooler.supabase.com:6543/postgres?pgbouncer=true&connection_limit=10"
NEW_DIRECT_URL="postgresql://postgres.NEWPROJECTID:NEWPASSWORD@aws-1-ap-southeast-2.pooler.supabase.com:5432/postgres"
EOF
```

**1.2. Deploy Schema to New Instance**
```bash
cd /app/backend

# Test connection
export $(grep "^NEW_DIRECT_URL=" .env | xargs)
psql "$NEW_DIRECT_URL" -c "SELECT version();"

# Deploy schema via Prisma
# Temporarily override DATABASE_URL
export DATABASE_URL="$NEW_DIRECT_URL"
npx prisma migrate deploy

# Or use db push (faster, no migration history)
npx prisma db push --skip-generate

# Verify tables created
psql "$NEW_DIRECT_URL" -c "\dt"
```

**1.3. Deploy RLS Policies to New Instance**
```bash
cd /app/backend

# Execute all RLS phases
export $(grep "^NEW_DIRECT_URL=" .env | xargs)
psql "$NEW_DIRECT_URL" -f prisma/rls-phase-1-revoke.sql
psql "$NEW_DIRECT_URL" -f prisma/rls-phase-2-enable.sql
psql "$NEW_DIRECT_URL" -f prisma/rls-phase-3a-basic-tables-fixed.sql
psql "$NEW_DIRECT_URL" -f prisma/rls-phase-3b-trip-product-fixed.sql
psql "$NEW_DIRECT_URL" -f prisma/rls-phase-3c-order-tables-fixed.sql
psql "$NEW_DIRECT_URL" -f prisma/rls-phase-4-grant.sql

# Verify RLS
psql "$NEW_DIRECT_URL" -f prisma/rls-verify-all.sql
```

---

#### ðŸŸ  PHASE 2: DATA MIGRATION (Optional, 30-60 minutes)

**If you need to migrate existing data:**

**2.1. Dump Data from Old Instance**
```bash
cd /app/backend

# Dump data only (no schema)
export $(grep "^DATABASE_URL=" .env | grep -v "NEW_" | xargs)
pg_dump "$DATABASE_URL" \
  --data-only \
  --no-owner \
  --no-acl \
  -f data_export_$(date +%Y%m%d).sql

# Or selective tables
pg_dump "$DATABASE_URL" \
  --data-only \
  --no-owner \
  --no-acl \
  -t "User" -t "Trip" -t "Product" -t "Order" \
  -f data_export_selective.sql
```

**2.2. Import Data to New Instance**
```bash
cd /app/backend

export $(grep "^NEW_DIRECT_URL=" .env | xargs)
psql "$NEW_DIRECT_URL" -f data_export_$(date +%Y%m%d).sql

# Check row counts
psql "$NEW_DIRECT_URL" -c "SELECT 
  'User' as table, COUNT(*) FROM \"User\" UNION ALL
  SELECT 'Trip', COUNT(*) FROM \"Trip\" UNION ALL
  SELECT 'Product', COUNT(*) FROM \"Product\" UNION ALL
  SELECT 'Order', COUNT(*) FROM \"Order\";"
```

**2.3. Alternative: Use Migration Script**
```bash
cd /app/backend

# Update migrate-data.ts to use NEW_DIRECT_URL
export LOCAL_DATABASE_URL="$DATABASE_URL"
export SUPABASE_DATABASE_URL="$NEW_DIRECT_URL"

npx tsx src/scripts/migrate-data.ts
```

---

#### ðŸŸ¢ PHASE 3: CONFIGURE OAUTH (15 minutes)

**3.1. Configure Google OAuth in New Supabase**
```bash
# In new Supabase dashboard:
# Authentication â†’ Providers â†’ Google

# Enable Google OAuth
# Add OAuth credentials:
# - Client ID: [from Google Cloud Console]
# - Client Secret: [from Google Cloud Console]

# Authorized redirect URLs:
# - https://NEWPROJECTID.supabase.co/auth/v1/callback
# - https://jastipin.me/auth/callback
# - http://localhost:3000/auth/callback (dev)
```

**3.2. Update Google Cloud Console**
```bash
# Go to: https://console.cloud.google.com
# APIs & Services â†’ Credentials â†’ OAuth 2.0 Client

# Add new authorized redirect URI:
# https://NEWPROJECTID.supabase.co/auth/v1/callback

# Save changes
```

---

#### ðŸ”µ PHASE 4: UPDATE APPLICATION CONFIG (10 minutes)

**4.1. Update Backend .env (CUTOVER)**
```bash
cd /app/backend

# Backup current .env
cp .env .env.old_supabase

# Replace old URLs with new ones
sed -i 's|DATABASE_URL="postgresql://postgres.ascucdkibziqamaaovqw|DATABASE_URL="postgresql://postgres.NEWPROJECTID|g' .env
sed -i 's|DIRECT_URL="postgresql://postgres.ascucdkibziqamaaovqw|DIRECT_URL="postgresql://postgres.NEWPROJECTID|g' .env

# Update password in URLs
# MANUALLY edit .env to replace password
nano .env
# Find: :123Empatlima@
# Replace with: :NEWPASSWORD@

# Verify changes
cat .env | grep "DATABASE_URL="
```

**4.2. Update Frontend .env.production**
```bash
cd /app/frontend

# Backup
cp .env.production .env.production.old_supabase

# Update Supabase URL
sed -i 's|ascucdkibziqamaaovqw|NEWPROJECTID|g' .env.production

# Update anon key
# MANUALLY replace NEXT_PUBLIC_SUPABASE_ANON_KEY
nano .env.production
# Replace with NEW_ANON_KEY from Phase 0.2
```

**4.3. Update Prisma Schema (Optional but Recommended)**
```bash
cd /app/backend

# Edit prisma/schema.prisma
nano prisma/schema.prisma

# Add directUrl:
# datasource db {
#   provider  = "postgresql"
#   url       = env("DATABASE_URL")
#   directUrl = env("DIRECT_URL")  # â† ADD THIS
# }

# Regenerate Prisma Client
npx prisma generate
```

---

#### ðŸŸ£ PHASE 5: TESTING & VERIFICATION (30 minutes)

**5.1. Test Backend Connection**
```bash
cd /app/backend

# Test Prisma connection
npx prisma db execute --stdin <<< "SELECT NOW();"

# Test connection pool
npm run build
NODE_ENV=production node dist/index.js &
SERVER_PID=$!

# Wait 5 seconds
sleep 5

# Test health endpoint
curl http://localhost:4000/health

# Stop server
kill $SERVER_PID
```

**5.2. Test Database Queries**
```bash
cd /app/backend

export $(grep "^DATABASE_URL=" .env | xargs)

# Test read
psql "$DATABASE_URL" -c "SELECT COUNT(*) as user_count FROM \"User\";"

# Test write
psql "$DATABASE_URL" -c "INSERT INTO \"FeesConfig\" (id, scope, calculationType, value) VALUES (gen_random_uuid()::text, 'test', 'flat', 1000);"

# Test RLS
psql "$DATABASE_URL" -c "SELECT tablename, rowsecurity FROM pg_tables WHERE schemaname='public' ORDER BY tablename;"
```

**5.3. Test Frontend OAuth**
```bash
cd /app/frontend

# Build with new env
npm run build

# Start server
npm start &
FRONTEND_PID=$!

# Open browser to: http://localhost:3000/auth
# Test Google login flow

# Check console for errors
# Should redirect to: https://NEWPROJECTID.supabase.co/auth/v1/authorize?...

# Stop server
kill $FRONTEND_PID
```

**5.4. Test OAuth Callback**
```bash
# After successful Google login, should:
# 1. Redirect to https://jastipin.me/auth/callback
# 2. Exchange code for session
# 3. Sync user to backend
# 4. Redirect to /dashboard

# Check browser console logs
# Check backend API logs
pm2 logs jastipin-api

# Verify user synced
psql "$DATABASE_URL" -c "SELECT id, email, slug FROM \"User\" ORDER BY \"createdAt\" DESC LIMIT 5;"
```

---

#### ðŸŸ¤ PHASE 6: PRODUCTION DEPLOYMENT (15 minutes)

**6.1. Deploy Backend**
```bash
cd /app/backend

# Push .env to production server
scp .env root@YOUR_SERVER:/app/backend/.env

# SSH to server
ssh root@YOUR_SERVER

# Restart backend
cd /app/backend
pm2 restart jastipin-api

# Check logs
pm2 logs jastipin-api --lines 50
```

**6.2. Deploy Frontend**
```bash
cd /app/frontend

# If using Vercel/Netlify, update env variables in dashboard
# Or deploy with new .env.production

# Rebuild & deploy
npm run build

# Deploy (example for custom server)
scp -r .next/ root@YOUR_SERVER:/app/frontend/.next/
ssh root@YOUR_SERVER "cd /app/frontend && pm2 restart jastipin-frontend"
```

**6.3. Verify Production**
```bash
# Test API
curl https://api.jastipin.me/health

# Test frontend
curl -I https://jastipin.me

# Test OAuth
# Open browser: https://jastipin.me
# Click "Sign in with Google"
# Should complete flow successfully
```

---

#### âš« PHASE 7: CLEANUP & MONITORING (24 hours)

**7.1. Monitor for 24 Hours**
```bash
# Watch backend logs
pm2 logs jastipin-api --lines 100

# Watch for errors
pm2 logs jastipin-api --err

# Check database connections
export $(cat /app/backend/.env | grep "^DATABASE_URL=" | xargs)
psql "$DATABASE_URL" -c "SELECT count(*) FROM pg_stat_activity WHERE datname='postgres';"
```

**7.2. Performance Check**
```bash
# Check slow queries
psql "$DATABASE_URL" -c "SELECT query, mean_exec_time, calls FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10;"

# Check connection pool usage
psql "$DATABASE_URL" -c "SELECT * FROM pg_stat_activity WHERE state='active';"
```

**7.3. Cleanup Old Instance (After 7 days)**
```bash
# If everything stable for 7 days:

# 1. Export final backup from old instance
# 2. Pause old Supabase project (Dashboard â†’ Settings â†’ Pause)
# 3. Wait 7 more days
# 4. Delete old Supabase project (if no issues)
```

---

## âš ï¸ ROLLBACK PROCEDURE

**If migration fails at any phase:**

### Quick Rollback (5 minutes)

**Backend:**
```bash
cd /app/backend
cp .env.old_supabase .env
pm2 restart jastipin-api
```

**Frontend:**
```bash
cd /app/frontend
cp .env.production.old_supabase .env.production
npm run build
pm2 restart jastipin-frontend
```

### Verify Rollback
```bash
curl https://api.jastipin.me/health
curl -I https://jastipin.me
# Test Google login
```

---

## ðŸ“‹ POST-MIGRATION CHECKLIST

- [ ] Backend connects to new database
- [ ] Frontend OAuth works with new Supabase
- [ ] User registration works
- [ ] User login (Google) works
- [ ] API queries return correct data
- [ ] RLS policies enforced
- [ ] No errors in logs
- [ ] Performance acceptable (< 5% slower)
- [ ] Backup created and verified
- [ ] Old instance still available for 7 days
- [ ] Team notified of migration
- [ ] Documentation updated

---

---

## ðŸ”§ WORKER SETUP - CRITICAL FOR QUEUE PROCESSING

### Worker Architecture

**Worker Process**: Standalone background process that consumes jobs from PGMQ (PostgreSQL Message Queue)

**Files**:
- `/app/backend/src/worker.ts` - Entry point
- `/app/backend/src/workers/queue-worker.ts` - Main worker loop
- `/app/backend/src/workers/job-handlers.ts` - Job execution logic
- `/app/backend/src/services/queue.service.ts` - PGMQ wrapper

**Queue Extension**: PGMQ v1.5.1 (already installed in Supabase âœ…)

### âš ï¸ CRITICAL: Worker Database Connection

**IMPORTANT**: Worker MUST use **DIRECT connection** (port 5432), NOT pooled connection (port 6543).

**Why?**
- PGMQ extension requires direct PostgreSQL connection
- PgBouncer (pooled, port 6543) does NOT support PGMQ operations
- Using pooled connection will cause: `pgmq.create()` errors, queue operations fail

**Solution**: Worker now automatically uses `DIRECT_URL` from `.env`

### Setup & Running Worker

#### 1. Verify PGMQ Extension Installed

```bash
cd /app/backend
export $(cat .env | grep "^DIRECT_URL=" | xargs)
psql "$DIRECT_URL" -c "\\dx pgmq"

# Expected output:
# pgmq | 1.5.1 | pgmq | PostgreSQL Message Queue
```

If not installed:
```sql
CREATE EXTENSION IF NOT EXISTS pgmq;
```

#### 2. Initialize Queue (First Time Only)

```bash
cd /app/backend

# Development
npm run worker:dev

# Production
npm run build
npm run worker:start
```

Worker will auto-create queue `jastipin_jobs` on first run.

#### 3. Run Worker (Development)

**Terminal 1 - API Server:**
```bash
cd /app/backend
npm run dev
# API running on http://localhost:4000
```

**Terminal 2 - Worker Process:**
```bash
cd /app/backend
npm run worker:dev
# Worker starts polling queue every 100ms
```

#### 4. Run Worker (Production with PM2)

**Start API:**
```bash
cd /app/backend
pm2 start dist/index.js --name jastipin-api
```

**Start Worker:**
```bash
cd /app/backend
pm2 start dist/worker.js --name jastipin-worker
pm2 save
```

**Monitor Both:**
```bash
pm2 list
pm2 logs

# Specific logs
pm2 logs jastipin-api
pm2 logs jastipin-worker
```

#### 5. Verify Worker is Running

**Check Process:**
```bash
pm2 status jastipin-worker

# Should show:
# â”‚ jastipin-worker â”‚ online â”‚ 1 â”‚ 0 â”‚ 0s â”‚ ...
```

**Check Logs:**
```bash
pm2 logs jastipin-worker --lines 50

# Expected output:
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#   Jastipin Queue Worker Starting...
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Environment: production
# Database: âœ“ Direct connection
# Process ID: 12345
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [WORKER] Initializing...
# [QUEUE] Initialized: jastipin_jobs
# [WORKER] Started âœ“
# [WORKER] Process ID: 12345
# [WORKER] Poll interval: 100ms
```

**Check Queue Stats:**
```bash
export $(cat .env | grep "^DIRECT_URL=" | xargs)
psql "$DIRECT_URL" -c "SELECT * FROM pgmq.metrics('jastipin_jobs');"

# Shows: queue_length, total_pop, total_read, etc.
```

#### 6. Test Queue (Send Test Job)

**Via API Endpoint** (if you have one):
```bash
curl -X POST http://localhost:4000/api/queue/test \
  -H "Content-Type: application/json" \
  -d '{"message":"test job"}'
```

**Via Direct SQL** (for testing):
```sql
-- Send test job to queue
SELECT pgmq.send(
  queue_name => 'jastipin_jobs',
  msg => '{"type":"test","payload":{"message":"hello"},"retryCount":0,"maxRetries":3}'::jsonb,
  delay => 0
);

-- Check queue has jobs
SELECT * FROM pgmq.metrics('jastipin_jobs');
```

**Watch Worker Logs:**
```bash
pm2 logs jastipin-worker --lines 100

# Should see:
# [WORKER] Processing job: test (msg_id=1, retry=0)
# [WORKER] Job completed: test (msg_id=1, duration=50ms)
```

### Worker Health Monitoring

**Health Check (Every 30 seconds):**
```
[HEALTH] {
  status: 'UP',
  uptime: '300s',
  activeJobs: 0,
  totalProcessed: 15,
  totalFailed: 0,
  queueStats: { queued: 0, processing: 0 }
}
```

**Graceful Shutdown:**
```bash
pm2 stop jastipin-worker

# Worker waits for active jobs to complete (max 30s)
# Then prints final stats and exits cleanly
```

### Worker Configuration

**Environment Variables:**
```env
# .env
DIRECT_URL="postgresql://postgres:PASSWORD@...5432/postgres"  # REQUIRED
NODE_ENV="production"
LOG_LEVEL="info"              # optional: debug, info, warn, error
POLL_INTERVAL_MS="100"        # optional: queue poll interval
```

**Worker Settings** (in `queue-worker.ts`):
```typescript
POLL_INTERVAL_MS = 100        // Poll queue every 100ms
HEALTH_CHECK_INTERVAL_MS = 30000  // Health log every 30s
GRACEFUL_SHUTDOWN_TIMEOUT_MS = 30000  // Wait 30s for jobs
```

**Queue Settings** (in `queue.service.ts`):
```typescript
QUEUE_NAME = 'jastipin_jobs'
VISIBILITY_TIMEOUT = 30       // Job lock duration (seconds)
MAX_RETRIES = 3               // Max retry attempts
BACKOFF_MS = [5000, 30000, 300000]  // 5s, 30s, 5min
```

### Troubleshooting Worker

#### Error: "pgmq.create() failed"
```
Cause: Using pooled connection (PgBouncer)
Solution: Set DIRECT_URL in .env
```

#### Error: "Queue not initialized"
```bash
# Manually initialize queue
export $(cat .env | grep "^DIRECT_URL=" | xargs)
psql "$DIRECT_URL" -c "SELECT pgmq.create('jastipin_jobs');"
```

#### Worker not processing jobs
```bash
# Check queue has jobs
psql "$DIRECT_URL" -c "SELECT * FROM pgmq.metrics('jastipin_jobs');"

# Check worker is running
pm2 status jastipin-worker

# Check worker logs
pm2 logs jastipin-worker --err
```

#### High CPU usage
```
Cause: POLL_INTERVAL_MS too low (< 50ms)
Solution: Increase to 100-500ms in worker.ts
```

#### Jobs stuck in queue
```bash
# Check visibility timeout
# If job processing takes > 30s, increase VISIBILITY_TIMEOUT

# Check for deadlocks
psql "$DIRECT_URL" -c "SELECT * FROM pg_stat_activity WHERE state='active';"
```

### Worker Scaling

**Horizontal Scaling** (Multiple Workers):
```bash
# Start multiple worker instances
pm2 start dist/worker.js --name jastipin-worker-1
pm2 start dist/worker.js --name jastipin-worker-2
pm2 start dist/worker.js --name jastipin-worker-3

# PGMQ handles job distribution automatically
# Each worker polls and locks jobs (VISIBILITY_TIMEOUT)
```

**Vertical Scaling** (Same process):
- Not recommended for PGMQ
- Use horizontal scaling instead

### Worker Maintenance

**Restart Worker:**
```bash
pm2 restart jastipin-worker
# Graceful shutdown â†’ wait for jobs â†’ restart
```

**Clear Queue (CAUTION):**
```sql
-- Deletes all jobs!
SELECT pgmq.purge('jastipin_jobs');
```

**View Dead Letter Queue:**
```sql
-- No DLQ yet (TODO in code)
-- Failed jobs are logged but not persisted
-- Consider adding custom DLQ table
```

---

**Report Updated**: 2025-12-01  
**Analyst**: backend-architect  
**Priority**: MEDIUM (migration prep)  
**Status**: âœ… WORKER SETUP COMPLETE - MIGRATION GUIDE READY
